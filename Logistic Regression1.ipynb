{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c48801",
   "metadata": {},
   "source": [
    "# Logistic Regression 1: Concepts, Evaluation, and Practical Issues\n",
    "This notebook covers the differences between linear and logistic regression, cost function and optimization, regularization, ROC curve, feature selection, handling imbalanced datasets, and common implementation challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f11c1a",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487eded",
   "metadata": {},
   "source": [
    "- **Linear regression** predicts a continuous outcome (e.g., house price) using a linear relationship between input features and the target.\n",
    "- **Logistic regression** predicts the probability of a binary outcome (e.g., spam vs. not spam) using the logistic (sigmoid) function.\n",
    "\n",
    "**Example:** Predicting whether a patient has a disease (yes/no) based on medical test results is more appropriate for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24499511",
   "metadata": {},
   "source": [
    "## Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae401948",
   "metadata": {},
   "source": [
    "The cost function is **log loss** (binary cross-entropy):\n",
    "\n",
    "Cost = -[y*log(p) + (1-y)*log(1-p)]\n",
    "\n",
    "where y is the true label and p is the predicted probability. The cost is minimized using optimization algorithms such as gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86096c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logistic regression with scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "print('Coefficients:', logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111c7ed",
   "metadata": {},
   "source": [
    "## Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef84e3",
   "metadata": {},
   "source": [
    "Regularization adds a penalty to the cost function to discourage large coefficients, helping to prevent overfitting. Common types:\n",
    "- **L1 (Lasso):** Encourages sparsity (some coefficients become zero).\n",
    "- **L2 (Ridge):** Shrinks coefficients but keeps all features.\n",
    "\n",
    "Regularization strength is controlled by the parameter C (inverse of regularization strength) in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logistic regression with L1 and L2 regularization\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "logreg_l2 = LogisticRegression(penalty='l2')\n",
    "logreg_l1.fit(X, y)\n",
    "logreg_l2.fit(X, y)\n",
    "print('L1 coefficients:', logreg_l1.coef_)\n",
    "print('L2 coefficients:', logreg_l2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfc2bd",
   "metadata": {},
   "source": [
    "## Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9e11c",
   "metadata": {},
   "source": [
    "The **ROC curve** (Receiver Operating Characteristic) plots the true positive rate (sensitivity) against the false positive rate at various threshold settings. It helps evaluate the model's ability to distinguish between classes. The area under the ROC curve (AUC) summarizes the model's performance; higher AUC indicates better discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ROC curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probs = logreg.predict_proba(X)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "auc = roc_auc_score(y, probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bda19",
   "metadata": {},
   "source": [
    "## Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d327f",
   "metadata": {},
   "source": [
    "- **L1 regularization (Lasso):** Automatically selects features by shrinking some coefficients to zero.\n",
    "- **Recursive Feature Elimination (RFE):** Iteratively removes least important features.\n",
    "- **Univariate statistical tests:** Select features based on statistical significance.\n",
    "\n",
    "These techniques reduce overfitting, improve interpretability, and may enhance model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4694fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature selection with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "selector = RFE(logreg, n_features_to_select=1)\n",
    "selector.fit(X, y)\n",
    "print('Feature ranking:', selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf41741",
   "metadata": {},
   "source": [
    "## Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf955f",
   "metadata": {},
   "source": [
    "- **Resampling:** Oversample minority class or undersample majority class.\n",
    "- **Class weights:** Use class_weight='balanced' in scikit-learn.\n",
    "- **Synthetic data:** Use SMOTE or similar techniques to generate synthetic samples.\n",
    "- **Evaluation metrics:** Use metrics like precision, recall, F1-score, and AUC instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712464f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using class weights\n",
    "logreg_balanced = LogisticRegression(class_weight='balanced')\n",
    "logreg_balanced.fit(X, y)\n",
    "print('Coefficients (balanced):', logreg_balanced.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d528add",
   "metadata": {},
   "source": [
    "## Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c6170",
   "metadata": {},
   "source": [
    "**Common issues:**\n",
    "- **Multicollinearity:** Use regularization (L1/L2), remove or combine correlated features, or use dimensionality reduction (PCA).\n",
    "- **Non-linearity:** Add interaction or polynomial terms, or use non-linear models.\n",
    "- **Outliers:** Remove or transform outliers.\n",
    "- **Imbalanced data:** Use resampling or class weights.\n",
    "- **Missing values:** Impute missing data before modeling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
