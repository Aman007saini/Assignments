{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97485ec1",
   "metadata": {},
   "source": [
    "# Logistic Regression 2: Model Selection, Data Leakage, and Confusion Matrix Analysis\n",
    "This notebook covers grid search CV, random search CV, data leakage, confusion matrix interpretation, and related classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77841da",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e342c",
   "metadata": {},
   "source": [
    "Grid search cross-validation (GridSearchCV) is used to find the best combination of hyperparameters for a model. It works by exhaustively searching over a specified parameter grid, training and evaluating the model for each combination using cross-validation, and selecting the combination with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5bc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "print('Best parameters:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2977b6",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8c55e",
   "metadata": {},
   "source": [
    "- **Grid search CV:** Tests all possible combinations in the parameter grid. More thorough but computationally expensive.\n",
    "- **Randomized search CV:** Samples a fixed number of parameter combinations at random. Faster and more efficient for large or continuous parameter spaces.\n",
    "\n",
    "Choose randomized search when the parameter space is large or when computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {'C': np.logspace(-3, 2, 100), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "random_search = RandomizedSearchCV(LogisticRegression(), param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "print('Best parameters (randomized):', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868af73e",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e9304",
   "metadata": {},
   "source": [
    "Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates. For example, if test data is used during feature engineering or model training, the model may \"cheat\" and perform unrealistically well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13701bbc",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb8ba7",
   "metadata": {},
   "source": [
    "- Split data into training and test sets before any preprocessing or feature engineering.\n",
    "- Use pipelines to ensure transformations are applied only to training data during cross-validation.\n",
    "- Avoid using future or target-related information in feature creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aee825",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4341d",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). It helps identify how well the model distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe586b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = grid.predict(X)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print('Confusion matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff35a3",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9a428",
   "metadata": {},
   "source": [
    "- **Precision:** TP / (TP + FP) — the proportion of positive predictions that are actually correct.\n",
    "- **Recall:** TP / (TP + FN) — the proportion of actual positives that are correctly identified.\n",
    "\n",
    "Precision focuses on prediction accuracy, while recall focuses on capturing all actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f5a52",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a98a02",
   "metadata": {},
   "source": [
    "By examining the off-diagonal elements (FP and FN), you can see whether the model is making more false positives or false negatives, and adjust the model or threshold accordingly based on the application's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291deb",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b174fd",
   "metadata": {},
   "source": [
    "- **Accuracy:** (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision:** TP / (TP + FP)\n",
    "- **Recall:** TP / (TP + FN)\n",
    "- **F1-score:** 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "These metrics provide different perspectives on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4de1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating metrics from confusion matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(y, y_pred))\n",
    "print('Precision:', precision_score(y, y_pred))\n",
    "print('Recall:', recall_score(y, y_pred))\n",
    "print('F1-score:', f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc9fdd",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc1769",
   "metadata": {},
   "source": [
    "Accuracy is the proportion of correct predictions (TP + TN) out of all predictions. It is directly calculated from the confusion matrix values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aae5c9",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8209f7c",
   "metadata": {},
   "source": [
    "If the model makes more errors for one class (e.g., more FNs than FPs), it may be biased or not generalizing well. Analyzing the confusion matrix helps identify such issues and guides further model tuning or data collection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
