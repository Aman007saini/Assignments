{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f83db71",
   "metadata": {},
   "source": [
    "# Support Vector Machines-2: Kernels, SVR, and SVC Assignment\n",
    "This notebook covers the relationship between polynomial and kernel functions, SVM with polynomial kernel, SVR parameters, and a full SVC classification workflow with hyperparameter tuning and model saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e181f4",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b85ead",
   "metadata": {},
   "source": [
    "Polynomial kernel functions allow algorithms like SVM to learn non-linear relationships by implicitly mapping input features into a higher-dimensional space using polynomial functions. The kernel trick computes the inner product in this space without explicit transformation, enabling efficient non-linear classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713beff7",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "svc_poly = SVC(kernel='poly', degree=3, C=1)\n",
    "svc_poly.fit(X, y)\n",
    "print('SVM with polynomial kernel trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45f751",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ebe4",
   "metadata": {},
   "source": [
    "In SVR (Support Vector Regression), increasing epsilon widens the margin of tolerance where no penalty is given to errors. As epsilon increases, fewer data points fall outside the margin, so the number of support vectors typically decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1965d",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb81c83",
   "metadata": {},
   "source": [
    "- **Kernel function:** Determines the shape of the decision boundary. Use 'linear' for linear data, 'poly' or 'rbf' for non-linear data.\n",
    "- **C parameter:** Controls the trade-off between margin width and training error. Higher C = less regularization (fit training data closely), lower C = more regularization (wider margin).\n",
    "- **Epsilon parameter:** Sets the width of the no-penalty zone in SVR. Larger epsilon = fewer support vectors, smoother fit.\n",
    "- **Gamma parameter:** For 'rbf' and 'poly' kernels, controls the influence of a single training example. Higher gamma = more complex model, lower gamma = smoother model.\n",
    "\n",
    "**Examples:**\n",
    "- Increase C for less regularization if underfitting; decrease C if overfitting.\n",
    "- Increase gamma for more complex boundaries; decrease for smoother boundaries.\n",
    "- Increase epsilon for a sparser model in SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc5208",
   "metadata": {},
   "source": [
    "## Q5. SVC Classification Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load dataset\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load dataset (wine dataset as example)\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf746e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "\n",
    "# Train tuned classifier on entire dataset\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "svc_tuned = SVC(**grid.best_params_)\n",
    "svc_tuned.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(svc_tuned, 'svc_tuned_model.joblib')\n",
    "print('Trained SVC model saved to svc_tuned_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
