{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd429ce",
   "metadata": {},
   "source": [
    "# Regression 2: Model Evaluation and Regularization\n",
    "This notebook covers R-squared, adjusted R-squared, regression error metrics, and regularization techniques (Lasso and Ridge), with practical examples and discussion of their use in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d06fd",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9d2ea",
   "metadata": {},
   "source": [
    "**R-squared** (coefficient of determination) measures the proportion of variance in the dependent variable that is predictable from the independent variables. It is calculated as:\n",
    "\n",
    "R² = 1 - (SS_res / SS_tot)\n",
    "\n",
    "where SS_res is the sum of squared residuals and SS_tot is the total sum of squares. R-squared values range from 0 to 1, with higher values indicating a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b74b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assume Y and model.predict(X) from previous example\n",
    "r2 = r2_score(Y, model.predict(X))\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5668029",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3433e",
   "metadata": {},
   "source": [
    "**Adjusted R-squared** adjusts the R-squared value for the number of predictors in the model. It penalizes the addition of irrelevant variables and is calculated as:\n",
    "\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "where n is the number of observations and p is the number of predictors. Unlike R-squared, adjusted R-squared can decrease if unnecessary variables are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18420b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating adjusted R-squared\n",
    "n = X.shape[0]\n",
    "p = X.shape[1] if len(X.shape) > 1 else 1\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"Adjusted R-squared: {adj_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0310d",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998d6e7",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps prevent overfitting by penalizing unnecessary variables, making it useful for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0abee",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e918f5",
   "metadata": {},
   "source": [
    "- **MSE (Mean Squared Error):** Average of squared differences between actual and predicted values.\n",
    "- **RMSE (Root Mean Squared Error):** Square root of MSE; measures average prediction error in the same units as the target.\n",
    "- **MAE (Mean Absolute Error):** Average of absolute differences between actual and predicted values.\n",
    "\n",
    "All three metrics measure model prediction error, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating RMSE, MSE, and MAE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(Y, model.predict(X))\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y, model.predict(X))\n",
    "print(f\"MSE: {mse:.2f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d5ae1",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b256e28",
   "metadata": {},
   "source": [
    "- **RMSE:** Sensitive to outliers; penalizes large errors more. Good for applications where large errors are especially undesirable.\n",
    "- **MSE:** Similar to RMSE but in squared units; less interpretable.\n",
    "- **MAE:** Less sensitive to outliers; easier to interpret.\n",
    "\n",
    "**Disadvantages:**\n",
    "- RMSE/MSE can be overly influenced by outliers.\n",
    "- MAE does not penalize large errors as strongly.\n",
    "\n",
    "Choose the metric based on the problem context and error tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ee6b9",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058732a0",
   "metadata": {},
   "source": [
    "**Lasso regularization** (L1) adds a penalty equal to the absolute value of coefficients to the loss function, encouraging sparsity (some coefficients become zero). **Ridge regularization** (L2) adds a penalty equal to the square of coefficients, shrinking them but rarely making them exactly zero.\n",
    "\n",
    "Use Lasso when feature selection is important; use Ridge when all features are expected to contribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lasso vs Ridge\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X, Y)\n",
    "print('Lasso coefficients:', lasso.coef_)\n",
    "\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X, Y)\n",
    "print('Ridge coefficients:', ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cd90c",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36da6a",
   "metadata": {},
   "source": [
    "Regularized models (Lasso, Ridge) add a penalty to large coefficients, discouraging overly complex models that fit noise in the training data. This helps improve generalization to new data.\n",
    "\n",
    "**Example:** In a model with many features, regularization reduces the impact of less important features, preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb654b",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5e8ef",
   "metadata": {},
   "source": [
    "- May underfit if regularization is too strong.\n",
    "- Not suitable for highly non-linear relationships.\n",
    "- Lasso may arbitrarily select among correlated features.\n",
    "\n",
    "Alternative models (e.g., tree-based) may perform better for complex or non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17857a7",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608da42",
   "metadata": {},
   "source": [
    "The choice depends on the application's tolerance for large errors. RMSE penalizes large errors more than MAE. If large errors are critical, prefer Model A if its MAE is also reasonable. If not, Model B may be better. Comparing different metrics directly can be misleading; ideally, compare both models using the same metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a9215",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3ccaa",
   "metadata": {},
   "source": [
    "Choose the model with better validation performance (e.g., lower RMSE or MAE on validation data). Ridge is better when all features are useful; Lasso is better for feature selection. Trade-offs include interpretability (Lasso) vs. stability (Ridge). The choice of regularization parameter also affects performance and should be tuned."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
