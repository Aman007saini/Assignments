{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e33805",
   "metadata": {},
   "source": [
    "# Regression 3: Ridge Regression Concepts and Applications\n",
    "This notebook explores Ridge Regression, its assumptions, parameter tuning, feature selection, handling of multicollinearity, categorical variables, coefficient interpretation, and use in time-series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdd978",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d26e",
   "metadata": {},
   "source": [
    "**Ridge Regression** is a type of linear regression that includes an L2 penalty (the sum of squared coefficients) to the loss function. This penalty term shrinks the coefficients, helping to prevent overfitting, especially when predictors are highly correlated.\n",
    "\n",
    "**Difference:** Ordinary least squares (OLS) minimizes the sum of squared residuals, while Ridge Regression minimizes the sum of squared residuals plus the L2 penalty. Ridge can handle multicollinearity better than OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ridge vs OLS\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, Y)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X, Y)\n",
    "print('OLS coefficients:', ols.coef_)\n",
    "print('Ridge coefficients:', ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb7d33",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e647ca",
   "metadata": {},
   "source": [
    "Ridge Regression shares the same assumptions as OLS regression:\n",
    "1. Linearity: Linear relationship between predictors and target.\n",
    "2. Independence: Observations are independent.\n",
    "3. Homoscedasticity: Constant variance of errors.\n",
    "4. Normality: Errors are normally distributed.\n",
    "\n",
    "Ridge does not require predictors to be uncorrelated (can handle multicollinearity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317f288",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1d4f8",
   "metadata": {},
   "source": [
    "The tuning parameter (lambda or alpha) controls the strength of regularization. It is typically selected using cross-validation, such as GridSearchCV or RidgeCV in scikit-learn, to find the value that minimizes validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Selecting alpha with cross-validation\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X, Y)\n",
    "print('Best alpha:', ridge_cv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d30af9",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd704a3",
   "metadata": {},
   "source": [
    "Ridge Regression does not perform feature selection in the strict sense because it shrinks coefficients but does not set them exactly to zero. All features remain in the model, but their influence may be reduced. For feature selection, Lasso (L1) is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f217886",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de85ca",
   "metadata": {},
   "source": [
    "Ridge Regression is effective in the presence of multicollinearity. The L2 penalty stabilizes coefficient estimates, reducing their variance and making the model more robust when predictors are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1fc3f",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb9a1b",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both types. Categorical variables must be encoded (e.g., one-hot encoding) before fitting the model, while continuous variables can be used directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Encoding categorical variables for Ridge Regression\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example data with a categorical feature\n",
    "house_data = pd.DataFrame({\n",
    "    'SquareFootage': [1000, 1500, 2000],\n",
    "    'Type': ['A', 'B', 'A'],\n",
    "    'Price': [200, 250, 300]\n",
    "})\n",
    "\n",
    "X_cat = pd.get_dummies(house_data[['SquareFootage', 'Type']], drop_first=True)\n",
    "Y_cat = house_data['Price']\n",
    "ridge_cat = Ridge(alpha=1.0)\n",
    "ridge_cat.fit(X_cat, Y_cat)\n",
    "print('Coefficients:', ridge_cat.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f1536",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e963b2",
   "metadata": {},
   "source": [
    "Coefficients in Ridge Regression represent the change in the target variable for a one-unit change in the predictor, holding other variables constant. However, due to regularization, coefficients are shrunk toward zero, so their absolute values are smaller than in OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ea4ab",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de72cc1",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series analysis by including lagged variables or time-based features as predictors. However, you must ensure that data is not randomly shuffled and that temporal order is preserved during model training and validation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
