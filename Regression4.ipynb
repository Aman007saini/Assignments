{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d08e24",
   "metadata": {},
   "source": [
    "# Regression 4: Lasso Regression Concepts and Applications\n",
    "This notebook covers Lasso Regression, its advantages for feature selection, coefficient interpretation, tuning parameters, handling of non-linear problems and multicollinearity, and comparison with Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d58dee",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab6ad8",
   "metadata": {},
   "source": [
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds an L1 penalty (sum of absolute values of coefficients) to the loss function. This penalty can shrink some coefficients to exactly zero, effectively performing feature selection. Unlike OLS (no penalty) and Ridge (L2 penalty), Lasso can eliminate irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e148110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, Y)\n",
    "print('Lasso coefficients:', lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e3863",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b52b2",
   "metadata": {},
   "source": [
    "The main advantage is that Lasso can shrink some coefficients to exactly zero, automatically selecting a subset of features and simplifying the model. This helps in identifying the most important predictors and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae22bd",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aa2ec",
   "metadata": {},
   "source": [
    "Coefficients that are non-zero indicate features that are important for predicting the target variable. Coefficients that are exactly zero mean the corresponding features are excluded from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8056b",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836a7e3",
   "metadata": {},
   "source": [
    "The main tuning parameter is **alpha** (the regularization strength). Higher alpha increases the penalty, leading to more coefficients being set to zero (stronger feature selection), but may underfit. Lower alpha reduces the penalty, including more features but may overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba21177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Effect of alpha in Lasso\n",
    "for a in [0.01, 0.1, 1]:\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso.fit(X, Y)\n",
    "    print(f'Alpha: {a}, Coefficients: {lasso.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f554168",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f71ac",
   "metadata": {},
   "source": [
    "Yes, by transforming the input features (e.g., using polynomial or interaction terms), Lasso can be applied to non-linear problems. The model remains linear in the parameters, but the features can represent non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lasso with polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "lasso_poly = Lasso(alpha=0.1)\n",
    "lasso_poly.fit(X_poly, Y)\n",
    "print('Lasso coefficients (polynomial):', lasso_poly.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddb770",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e41e3",
   "metadata": {},
   "source": [
    "- **Ridge Regression** uses an L2 penalty (squared coefficients), shrinking coefficients but rarely making them exactly zero. All features remain in the model.\n",
    "- **Lasso Regression** uses an L1 penalty (absolute coefficients), which can shrink some coefficients to zero, performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e8a81",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245155cc",
   "metadata": {},
   "source": [
    "Yes, Lasso can handle multicollinearity by selecting one variable from a group of highly correlated features and setting the others to zero. However, the choice among correlated features may be arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96990e76",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48aaaf",
   "metadata": {},
   "source": [
    "The optimal value of alpha (lambda) is typically chosen using cross-validation (e.g., LassoCV in scikit-learn), selecting the value that minimizes validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Choosing alpha with cross-validation\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(alphas=[0.01, 0.1, 1, 10], cv=5)\n",
    "lasso_cv.fit(X, Y)\n",
    "print('Best alpha:', lasso_cv.alpha_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
