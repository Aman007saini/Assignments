{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd861488",
   "metadata": {},
   "source": [
    "# Regression 1: Linear, Multiple, and Polynomial Regression\n",
    "This notebook covers key concepts in regression analysis, including simple and multiple linear regression, assumptions, interpretation, gradient descent, multicollinearity, and polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1679f",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c910d7b",
   "metadata": {},
   "source": [
    "**Simple linear regression** models the relationship between a single independent variable (X) and a dependent variable (Y) using a straight line: Y = b0 + b1*X.\n",
    "\n",
    "**Example:** Predicting house price (Y) based on square footage (X).\n",
    "\n",
    "**Multiple linear regression** models the relationship between two or more independent variables (X1, X2, ...) and a dependent variable (Y): Y = b0 + b1*X1 + b2*X2 + ...\n",
    "\n",
    "**Example:** Predicting house price (Y) based on square footage (X1), number of bedrooms (X2), and age of the house (X3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff198f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example data\n",
    "X = np.array([1000, 1500, 2000, 2500, 3000]).reshape(-1, 1)\n",
    "Y = np.array([200, 250, 300, 350, 400])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "plt.scatter(X, Y, color='blue')\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('House Price (in $1000s)')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba879543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression example\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "house_data = pd.DataFrame({\n",
    "    'SquareFootage': [1000, 1500, 2000, 2500, 3000],\n",
    "    'Bedrooms': [2, 3, 3, 4, 4],\n",
    "    'Age': [10, 5, 8, 3, 1],\n",
    "    'Price': [200, 250, 300, 350, 400]\n",
    "})\n",
    "\n",
    "X_multi = house_data[['SquareFootage', 'Bedrooms', 'Age']]\n",
    "Y_multi = house_data['Price']\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_multi, Y_multi)\n",
    "print('Coefficients:', model_multi.coef_)\n",
    "print('Intercept:', model_multi.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b73b",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74190a54",
   "metadata": {},
   "source": [
    "**Assumptions of linear regression:**\n",
    "1. Linearity: Relationship between independent and dependent variables is linear.\n",
    "2. Independence: Observations are independent.\n",
    "3. Homoscedasticity: Constant variance of errors.\n",
    "4. Normality: Errors are normally distributed.\n",
    "5. No multicollinearity (for multiple regression).\n",
    "\n",
    "**How to check:**\n",
    "- Linearity: Plot residuals vs. fitted values.\n",
    "- Independence: Check data collection process.\n",
    "- Homoscedasticity: Plot residuals vs. fitted values.\n",
    "- Normality: Q-Q plot of residuals.\n",
    "- Multicollinearity: Calculate VIF (Variance Inflation Factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking assumptions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_const = sm.add_constant(X)\n",
    "model_sm = sm.OLS(Y, X_const).fit()\n",
    "residuals = model_sm.resid\n",
    "\n",
    "# Q-Q plot for normality\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. fitted values\n",
    "plt.scatter(model_sm.fittedvalues, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5554a",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54676157",
   "metadata": {},
   "source": [
    "- **Slope:** The change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
    "- **Intercept:** The expected value of Y when X = 0.\n",
    "\n",
    "**Example:** In a model predicting salary (Y) based on years of experience (X), if the slope is 5,000, each additional year of experience increases salary by $5,000. The intercept is the predicted salary for zero years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87676297",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5857a4",
   "metadata": {},
   "source": [
    "**Gradient descent** is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts model parameters in the direction of the steepest decrease in the cost function until convergence. It is widely used for training linear regression, logistic regression, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5801e",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81143d06",
   "metadata": {},
   "source": [
    "**Multiple linear regression** models the relationship between a dependent variable and two or more independent variables. The equation is:\n",
    "Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn\n",
    "\n",
    "It differs from simple linear regression, which uses only one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb66b9",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7ff10",
   "metadata": {},
   "source": [
    "**Multicollinearity** occurs when independent variables in a multiple regression model are highly correlated. This can make coefficient estimates unstable.\n",
    "\n",
    "**Detection:**\n",
    "- Calculate Variance Inflation Factor (VIF) for each variable. VIF > 5 or 10 indicates multicollinearity.\n",
    "\n",
    "**Addressing:**\n",
    "- Remove or combine correlated variables.\n",
    "- Use dimensionality reduction (e.g., PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculating VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_multi.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_multi.values, i) for i in range(X_multi.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5350ad",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396722",
   "metadata": {},
   "source": [
    "**Polynomial regression** models the relationship between the independent variable and the dependent variable as an nth degree polynomial. It can capture non-linear relationships, unlike linear regression which fits a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, Y)\n",
    "plt.scatter(X, Y, color='blue')\n",
    "plt.plot(X, model_poly.predict(X_poly), color='green')\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('House Price (in $1000s)')\n",
    "plt.title('Polynomial Regression (Degree 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88649c14",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d9765",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- Can model non-linear relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Prone to overfitting with high-degree polynomials.\n",
    "- Less interpretable than linear regression.\n",
    "\n",
    "**Use polynomial regression** when the relationship between variables is clearly non-linear and cannot be captured by a straight line."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
